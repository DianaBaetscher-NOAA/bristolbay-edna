---
title: "01-dada2"
output: html_notebook
---


18 January 2023

Processing primer-trimmed reads from Bristol Bay 2022 eDNA samples 
analyzed using metabarcoding with the salmonid Cyt B primer set.



```{r load-libraries}
library(dada2)
library(dplyr)


# file location
path <- "/genetics/edna/workdir/bristolbay/trimmed"

path
#list.files(path)
```


```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

```{r}
plotQualityProfile(fnFs[1:4])
plotQualityProfile(fnRs[1:4])
```
Yikes. The quality on these doesn't look great.
Based on these plots, I'd trim FWD reads at 100 bp and REV at 90 bp. Hopefully that's enough overlap to combine FWD + REV.



```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Initial filtering truncLen failed miserably. We changed it to be super-stringent to see how that affects downstream processing.
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(110,100),
              maxN=0, maxEE=c(2,4), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
head(out)
```

If there are files with no reads, we must remove them from the file list - otherwise the dada2 algorithm (below) cannot be implemented.

```{r}
out %>%
  as.data.frame() %>%
  filter(reads.out <1)

# filter the matrix to retain samples with >0 reads

```


```{r}
# tosser.names <- rownames(tossers)
# # move a set of files
# file.copy(from=file.path(paste0(path,"/", tosser.names)),
#           to=file.path(paste0(path, "/tossers","/", tosser.names)))
# # remove those from the original directory
# file.remove(from=file.path(paste0(path,"/",tosser.names)))

```



Let's move fwd with these for now... and come back if there are other issues.

### Error rates
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)

errR <- learnErrors(filtRs, multithread=TRUE)

```

```{r}
plotErrors(errF, nominalQ=TRUE)


plotErrors(errR, nominalQ=TRUE)

```

### Sample inference

```{r}
# forwards
dadaFs <- dada(filtFs, err=errF, pool="pseudo", multithread=TRUE)


# reverses
dadaRs <- dada(filtRs, err=errR, pool="pseudo", multithread=TRUE)

```

Merge paired end reads
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

Make a sequence table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
Let's remove the singletons and off-target sequences
```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 183:187]
```
using the prior trimming (100, 90), the size was 163 rather than 183...
expected fragment length (according to Damien Menning) is ~187 bp.




Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Calculate frequency of chimeras
```{r}
sum(seqtab.nochim)/sum(seqtab2)

```


Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

# output that info for the initial trimming (100, 90)
track %>%
  write.csv("csv_outputs/trimmed_110_100_reads.csv")
```

Hmmm, there are weird drops in the merged and non-chimeric samples. I made the merging parameters more lenient and increased the potential number of errors, particularly in the R2 data. Now it's just strange drops for chimeric sequences for some samples (but not all).

I almost wonder if using just the FWD reads would be more successful. There's also always the question of whether there was an issue with the library prep (beads or ethanol left in the final elution).


## Export files for taxonomy and samples/ASVs

```{r regseqs-asv-output}
 #make fasta file with ASVs
    asv_seqs=colnames(seqtab.nochim)
    for(i in 1:length(asv_seqs))
    {
        write.table(paste(">ASV",i, sep=""),file="csv_outputs/bristolbay_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
        write.table(paste(asv_seqs[i], sep=""),file="csv_outputs/bristolbay_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
    }
```

That's the input for the FASTA blastn search.


```{r first-for-poolseqs}
# Make map between brief names and full sequences
briefToSeq <- colnames(seqtab.nochim)
names(briefToSeq) <- paste0("ASV", seq(ncol(seqtab.nochim))) # Seq1, Seq2, ...
# Make new sequence table with brief names
st.brief <- seqtab.nochim
colnames(st.brief) <- names(briefToSeq)

# export the pool seq table with brief names:
write.csv(st.brief, file="csv_outputs/bristolbay_ASVtable.csv")
```

